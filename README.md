# Mi Primer Proyecto Data Engineer ğŸš€

Este es mi primer proyecto completo de Data Engineering, implementando un pipeline de datos end-to-end usando herramientas gratuitas y estÃ¡ndar del mercado, con Python como lenguaje principal.

## ğŸ› ï¸ Stack TecnolÃ³gico
- **Python 3.13** (lenguaje principal)
- **SQLite** (base de datos local)
- **Streamlit** (dashboard interactivo)
- **Requests** (descarga de datos)
- **CSV + SQLite3** (procesamiento nativo de Python)

## ğŸ—ï¸ Arquitectura del Pipeline
1. **Ingesta**: Descarga automÃ¡tica de dataset pÃºblico (Uber rides)
2. **ValidaciÃ³n**: VerificaciÃ³n bÃ¡sica de estructura y calidad de datos
3. **TransformaciÃ³n**: Limpieza y procesamiento (muestra de 100 registros)
4. **Almacenamiento**: Carga en base de datos SQLite local
5. **VisualizaciÃ³n**: Dashboard web con Streamlit

## ğŸ“ Estructura del Proyecto
```
dataengineer1st/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Datos originales descargados
â”‚   â”œâ”€â”€ processed/              # Datos transformados
â”‚   â””â”€â”€ warehouse/              # Base de datos SQLite
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingest/                 # MÃ³dulo de ingesta de datos
â”‚   â”œâ”€â”€ quality/                # Validaciones de calidad
â”‚   â”œâ”€â”€ transform/              # Transformaciones de datos
â”‚   â”œâ”€â”€ warehouse/              # Carga a base de datos
â”‚   â””â”€â”€ orchestrate/            # Pipeline principal
â”œâ”€â”€ dash/                       # Dashboard de Streamlit
â”œâ”€â”€ tests/                      # Pruebas automatizadas
â””â”€â”€ requirements.txt            # Dependencias del proyecto
```

## ğŸš€ CÃ³mo Ejecutar el Proyecto

### 1. ConfiguraciÃ³n del Entorno
```powershell
# Crear entorno virtual
py -m venv .venv

# Activar entorno (si tienes permisos)
.\.venv\Scripts\Activate.ps1

# O cambiar polÃ­tica de ejecuciÃ³n
Set-ExecutionPolicy -Scope CurrentUser -ExecutionPolicy RemoteSigned
```

### 2. Instalar Dependencias
```powershell
# Instalar paquetes necesarios
.\.venv\Scripts\python -m pip install requests streamlit
```

### 3. Ejecutar Pipeline de Datos
```powershell
# Correr el pipeline completo (descarga, procesa y carga datos)
.\.venv\Scripts\python -m src.orchestrate.flow
```

### 4. Lanzar Dashboard
```powershell
# Abrir dashboard interactivo en el navegador
.\.venv\Scripts\streamlit run dash/app.py
```

## ğŸ“Š Resultados del Pipeline
- **Datos procesados**: 1,511,443 registros de viajes de Uber
- **Muestra analizada**: 100 registros para demostraciÃ³n
- **Columnas**: Date/Time, Lat, Lon
- **Base de datos**: SQLite con tabla `staging_trips`

## ğŸ¯ CaracterÃ­sticas Implementadas
- âœ… **Ingesta automatizada** de datos pÃºblicos
- âœ… **ValidaciÃ³n de calidad** bÃ¡sica
- âœ… **TransformaciÃ³n** y limpieza de datos
- âœ… **Almacenamiento** en base de datos local
- âœ… **Dashboard interactivo** con mÃ©tricas
- âœ… **CÃ³digo en espaÃ±ol** para facilitar comprensiÃ³n
- âœ… **Arquitectura modular** y escalable

## ğŸ”§ TecnologÃ­as Utilizadas y Por QuÃ©
- **Python**: Lenguaje estÃ¡ndar en Data Engineering
- **SQLite**: Base de datos ligera, perfecta para prototipos
- **Streamlit**: Framework rÃ¡pido para dashboards de datos
- **CSV nativo**: Evita dependencias complejas como pandas/pyarrow
- **Requests**: LibrerÃ­a estÃ¡ndar para descargas HTTP

## ğŸ“ˆ PrÃ³ximos Pasos para Expandir
1. **MÃ¡s datasets**: Agregar NYC Taxi, Chicago Bikes, etc.
2. **OrquestaciÃ³n**: Implementar Airflow o Prefect
3. **Cloud**: Migrar a AWS S3 + Redshift o GCP BigQuery
4. **Monitoreo**: Agregar logs y alertas de calidad
5. **Tests**: Ampliar cobertura de pruebas automatizadas

## ğŸ’¼ Para mi CV
Este proyecto demuestra:
- Conocimiento de arquitecturas de datos end-to-end
- Uso de Python para ETL/ELT
- ImplementaciÃ³n de pipelines automatizados
- CreaciÃ³n de dashboards de datos
- Buenas prÃ¡cticas de estructura de cÃ³digo
- Capacidad de trabajar con datos reales

---
**Autor**: Pablo  
**Fecha**: Octubre 2025  
**Objetivo**: Primer proyecto Data Engineer para portfolio profesional
